# Financial Reward on Users' Response Rate by Lin He
### **Why treatment group with $10 rewards has a 30% response rate while control group without rewards has a 50% response rate**

## Framework
1. Confirm: metrics meaning, if it is a statistically sound and robust finding
2. Restate the problem with the overall goal statement
3. Check experiment design: how do users know there is an reward? how do users claim the award? Is the award in cash or store credit?
4. User experience and characteristics including device, region, language and user id
5. Summary

## Execution steps:
(step 1) First I would like to understand the context better? What do you mean by response rate? Does it mean completion rate or click through rate?
(An: completion rate)
May I assume we have finished running all samples or do we take a peek at the data before the experiment is fully completed? 
May I assume sample size is the same between the two groups?
(An: statistically sound and robust)

(step 2) All right. It sounds like the finding wasn't what we expected. Our hypothese was monetary incentive will increase response rate in a statistically significant way,
but the experiment showed us the opposite result.

(step 3) If that's the case, I would like to double click on the experiment design. How did users know there was an award after the survey? Did they know it before hand or
did they only know it after they clicked on the survey? Did we accidentally introduce selection bias when users knew beforehand there is an reward and therefore incidentally 
created two user bases with dissimilar characteristics. Did we use notification with subject line that made the treatment group feel like it was a spam? 
After the survey, how easy was it for users to claim the reward? Were they not comfortable with the info they needed to fill and even abandoned the survey all together?
Was the award in cash or could only be used in certain stores?

(step 4) Next I would like to take a closer look at the two groups to understand their experience and characteristics better. What's the average duration to complete the survey?
Percentage of users who fill the optional free text boxes if any. Latency. Mobile or computer. User name or id to impute language. 

(step 5) In summary, I confirmed the meaning of the metric, and the result was valid. I then investigated the experiment design to make sure we didn't accidentally introduce
selection bias, the notification could be distinguished from spam, and that claiming reward wouldn't cause users to abandon survey. After checking the design, I studied the two 
groups to make sure they have similar experience in terms of latency. I also validated the hypothesis that reward didn't cause extra time other than the time spent in claiming it. 
Lastly, I checked device type and inferred language from user name/id to segment results in different sub groups. It is possible that the reward would still work for mobile user, 
for example, as they could put it into use easier, and users from certain underrepresented groups are more motivated by monetary reward. If that's the sample we would like to keep,
I would at least keep the reward for these groups.
